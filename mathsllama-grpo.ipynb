{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPwFBEs1mvV"
      },
      "source": [
        "# Goal\n",
        "\n",
        "In this notebook, we will use GRPO to enable the reasoning capability for Llama3.1, but this can be applied to any LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ygWmQJxKax"
      },
      "source": [
        "\n",
        "# SFT for Reasoning? ü§î\n",
        "Can we use supervised finetuning to enable a model to reason? Yes! But it requires\n",
        "\n",
        "- (question, good reasoning, good answer) triplets generated by a reasoning-capable model\n",
        "- fine-tuning our target model with above triplets\n",
        "\n",
        "Effectively we are distilling the knowledge and reasoning capability of a bigger and more capable model into a smaller model.\n",
        "\n",
        "While this approach is straightforward, we are bounded by the limits of an existing reasoning model. This can be a challenge if we want reasoning for a particular domain say medical or law, which the teacher model may not be good at."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeEY90JBxKay"
      },
      "source": [
        "# Reasoning from scratch? ü§î\n",
        "So how do we get a model to reason without relying on another model to guide it? How can we teach it to reason without feeding it tons of examples of ‚Äúgood‚Äù reasoning? Well, hiring annotators to write out solid reasoning for every hard question just isn‚Äôt practical or scalable. But here‚Äôs an alternative: instead of showing the model how to reason, we define what good reasoning looks like.\n",
        "\n",
        "üß© given ‚Üí üßë‚Äçüéì thinks ü§î ‚Üí answer ‚úÖ or ‚ùå ‚Üí improves via üß†üîÑ\n",
        "- Think of it like teaching someone to solve a puzzle without showing them the solution. Instead of walking them through the steps, you just tell them whether they got it right or wrong at the end. Over time, they have to figure out what kinds of moves or thought patterns lead to success. That‚Äôs the core idea behind reinforcement learning (RL): the model doesn‚Äôt get a play-by-play‚Äîit just gets a signal about whether the outcome was good or bad, and it has to learn the in-between steps on its own.\n",
        "\n",
        "üßë‚Äçüç≥ cooks üçù ‚Üí gets üëÉüëÄüëÖ feedback ‚Üí learns with üîÅüß†\n",
        "- Or think of it like teaching a newbie chef to cook. Instead of giving them recipes, we let them experiment in the kitchen‚Äîand then we give feedback: ‚ÄúDoes it look appetizing? Does it taste or smell good?‚Äù The chef has to figure out what steps lead to that tasty result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPIYZOcxKay"
      },
      "source": [
        "\n",
        "# The Magic ‚ú®\n",
        "All we need is a simple SFT dataset with just questions and answers. üöÄ\n",
        "\n",
        "Here‚Äôs the idea: We ask the model to solve a problem after reasoning through it, and we reward it only if the final solution is correct and it provides a proper reasoning flow. That‚Äôs it‚Äîno extra complexity!\n",
        "\n",
        "How It Works:\n",
        "Let‚Äôs say we give the model a hard math problem. Instead of jumping straight to the answer, we set up the system prompt so that the model:\n",
        "\n",
        "- Starts by reasoning through the problem and possible solutions.\n",
        "\n",
        "- Wraps the reasoning between \\<reasoning> and \\</reasoning> tags.\n",
        "\n",
        "- Provides the final solution only after reasoning is complete, placing it between \\<answer> and \\</answer> tags.\n",
        "\n",
        "We reward the model if:\n",
        "\n",
        "&nbsp;&nbsp;üèÜ The answer is correct‚Äîobviously, that‚Äôs a must!\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;üèÜ The final solution is properly tagged with \\<answer>.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;üèÜ The reasoning is properly tagged with \\<reasoning>.\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;üèÜ The reasoning meets a certain length requirement (to avoid shortcuts).\n",
        "\n",
        "\n",
        "It‚Äôs like telling a student: ‚ÄúI won‚Äôt just grade your final answer‚Äîyou need to show your work!‚Äù üéì‚ú®\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMoBBXU6xKaz"
      },
      "source": [
        "# RL Requires Reward üí∞\n",
        "RL relies on a reward signal. In the context of LLMs, we need a way to determine which completions or reasoning are good and which ones aren't as great. To generate the reward signal, one can\n",
        "\n",
        "&nbsp;&nbsp; üëâ use a reward model, which has been trained in advance to mimic human preferences\n",
        "\n",
        "&nbsp;&nbsp; üëâ design a set of reward functions that check out model completions align with our preferences (e.g., answer length, answer format)\n",
        "\n",
        "\n",
        "In short, the goal is to teach the model what \"good\" looks like so it can improve its responses over time! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5vyT2kFxKaz"
      },
      "source": [
        "# The Art of Dynamic Rewards üéØ\n",
        "One key aspect of RL is that rewards shouldn‚Äôt just reinforce consistency‚Äîthey should also push for growth. Imagine a student who starts off getting B‚Äôs‚Äîat first, the teacher might reward them to encourage progress. But if they keep getting B‚Äôs forever, the teacher needs to raise the bar, rewarding them only when they improve to an A or A+. In other words, rewards should be dynamic and adaptive, reinforcing positive surprises rather than maintaining the status quo. This ensures that learning doesn‚Äôt plateau but instead keeps progressing.\n",
        "\n",
        "\n",
        "Some popular RL-based training methods are Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and GRPO. Let's see how each of these 3 approaches adjust the reward for model's continous improvemnt.\n",
        "\n",
        "- PPO, a traditional RL algorithm, relies on a critic network to estimate this baseline reward for an LLM. PPO is like a traditional classroom, where the teacher (the critic network) evaluates every student not just on their latest assignment, but based on their overall performance history. The teacher has an internal benchmark for each student‚Äîif a consistent B student scores another B, it‚Äôs expected so not a large reward assigned. But if that same student suddenly earns an A+, that‚Äôs a big deal and deserves extra recognition. On the flip side, if a straight-A student suddenly drops to a B, the teacher knows something went wrong. This structured approach helps maintain steady progress, but it also makes learning complex and slow because the teacher has to track every student‚Äôs past performance, adjusting expectations individually for each one.\n",
        "\n",
        "\n",
        "- DPO simplifies learning by replacing the critic with direct preference comparisons, where the model learns by evaluating pairs of outputs ranked by human feedback. This pairwise comparison helps establish a relative reward context‚Äîsimilar to a student competing with a peer. If both students consistently receive similar grades, neither is performing exceptionally well; true excellence is only recognized when one significantly outperforms the other. However, this approach depends on a carefully curated preference dataset and is inherently limited by its two-output comparison framework.\n",
        "\n",
        "- GRPO simplifies learning by replacing the critic with a group-based comparison approach, where the model generates and evaluates multiple outputs at once rather than relying on pairwise comparisons. This enables the model to learn which responses are superior without needing a critic and without being limited to one-on-one assessments. An analogy would be evaluating a student‚Äôs performance relative to the entire class rather than just a single peer‚Äîproviding a broader and more informative benchmark for assessing excellence. Also, unlike DPO, we don't need to create a preference dataset in advance - we let the model generate multiple answers for a given question and each answer will get a reward from the reward function and then adjust the rewards based on the mean and variance of these rewards - no need to involve human feedback!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B5f80RWxKaz"
      },
      "source": [
        "# üî≠ GRPO at High-Level\n",
        "\n",
        "1. Group Sampling: For a single prompt, the policy (our LLM) generates a batch of completions (instead of just one). This produces a small ‚Äúgroup‚Äù of possible actions or answers.\n",
        "\n",
        "2. Reward Scoring: Each output is scored by a series of reward functions, which reflect how good or desirable that output is for the task at hand.\n",
        "\n",
        "3. Group-Based Advantage: The algorithm calculates each output‚Äôs ‚Äúadvantage‚Äù by comparing its reward to the average reward of the entire group (see the dynamic reward section). If the output‚Äôs reward is above average, it has a positive advantage (and vice versa).\n",
        "\n",
        "4. Policy Update: The policy is adjusted to promote outputs with a positive advantage and discourage those with a negative advantage.\n",
        "- A KL penalty term helps keep the policy from drifting too far from the original model. Without it, the model might learn to reason better, but at the cost of forgetting or degrading other important capabilities that aren't directly related to reasoning.\n",
        "\n",
        "5. Iterative Process: The updated policy is used again to generate new groups, score them, and update‚Äîrepeating until the policy converges or meets performance goals.\n",
        "\n",
        "This group-based approach removes the need for a separate value function (critic) and helps the policy quickly learn which outputs are relatively better within each sampled group.\n",
        "\n",
        "We will take a deeper dive into GRPO in the following section! üîç üåä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2994ydz1mvZ"
      },
      "source": [
        "### Installation\n",
        "\n",
        "As you can see, we'll only need a few dependencies thanks to the collective hard work of the community!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioBFUk9g1mvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed83675-3902-4f57-a571-96a178cdac1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: transformers==4.50.1 in /usr/local/lib/python3.11/dist-packages (4.50.1)\n",
            "Requirement already satisfied: diffusers==0.32.2 in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: trl==0.17.0 in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: bitsandbytes==0.45.4 in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.5.2) (0.5.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.1) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.32.2) (8.7.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.17.0) (3.6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl==0.17.0) (13.9.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.17.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.17.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.17.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.17.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.17.0) (0.70.15)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.5.2) (1.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.1) (2025.4.26)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.32.2) (3.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl==0.17.0) (2.19.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (3.11.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.17.0) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.17.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.17.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.17.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.17.0) (1.20.0)\n"
          ]
        }
      ],
      "source": [
        "# create a conda environment: conda create -n grpo_test python=3.12.4\n",
        "# activate conda environment in your colab notebook\n",
        "# It took me a while to figure out the version of torch that works with bitsandbytes==0.45.4\n",
        "!pip install torch==2.6.0 accelerate==1.5.2 transformers==4.50.1 diffusers==0.32.2 wandb pillow trl==0.17.0 bitsandbytes==0.45.4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check cuda version\n",
        "\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQCP_t3cL9gI",
        "outputId": "c7b36be2-fdef-48ba-f36a-e23b3c63dd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXceCzEcxKa1"
      },
      "source": [
        "# Model\n",
        "Let's set up the model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJCAafzcxKa1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AErXbL0RxKa1"
      },
      "outputs": [],
      "source": [
        "#  load the model's tokenizer and properly set padding token\n",
        "from transformers import AutoTokenizer\n",
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" #@param {type:\"string\"}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
        "\n",
        "# Setting the padding token is crucial for batch processing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # or choose another suitable token depending on the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flash-attn==2.6.3 --upgrade\n"
      ],
      "metadata": {
        "id": "_DOSEWwMK1lC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, flash_attn\n",
        "# print(torch.__version__, flash_attn.__version__)\n",
        "# from transformers.modeling_flash_attention_utils import _flash_supports_window_size\n",
        "# print(\"window flag:\", _flash_supports_window_size)\n"
      ],
      "metadata": {
        "id": "_wmOXbUoNN-k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AVc6vKGvxKa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513,
          "referenced_widgets": [
            "83a67a8aae4a42b1815663f1ded9e027",
            "0149d407f8ee40bb8f3caf0ad88d37b7",
            "ed3c465c550c44a1ab0e8015efdc40fc",
            "4e4f3ef2b3224c85afd8f4e49d8bdb80",
            "0b2929ca228442bdb7eb1f2a9985aa2e",
            "539b50bc7f53458eafd0a760e57fb986",
            "9fd4eb548d994dc1935edc1647c77d4b",
            "7f7089bab69e4166bed11ce5bf4c362d",
            "711e44021c684adcbcde3fa287951cf2",
            "9a880e478e0d439c8e73afb9ee90868a",
            "e72b75bcd46649d8a35c236a3bedf0c4"
          ]
        },
        "outputId": "ff444622-4dc7-49b6-beda-2fc83b4d6044"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83a67a8aae4a42b1815663f1ded9e027"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load model in NF4 and properly set torch_dtype and quantization_config\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "# Set up quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Load model weights in 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use NF4 quantization type\n",
        "    bnb_4bit_use_double_quant=True,  # Use double quantization\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for computation\n",
        ")\n",
        "\n",
        "# Load the model with the quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,  # Explicitly set torch_dtype\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        ")\n",
        "\n",
        "# Ensure the model is in training mode\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BL8mGGRRxKa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bf1da7-a09b-4304-eabe-b95279494c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
          ]
        }
      ],
      "source": [
        "#  Apply LoRA to the 4-bit model\n",
        "from peft import LoraConfig, get_peft_model\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA attention dimension\n",
        "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Do not train bias\n",
        "    task_type=\"CAUSAL_LM\",  # Task type is causal language modeling\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # Target modules for LoRA\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y53wbd2rxKa2"
      },
      "source": [
        "# Dataset\n",
        "For this project, we utilize the GSM8K (Grade School Math 8K) dataset, which consists of 8,500 carefully curated grade school math word problems characterized by linguistic diversity and high quality. GSM8K was specifically developed to facilitate question-answering tasks that involve basic math problems requiring multi-step reasoning. https://huggingface.co/datasets/openai/gsm8k\n",
        "\n",
        "One fundamental difference between Supervised Fine-Tuning (SFT) and Reinforcement Learning with GRPO lies in how inputs and outputs are structured during training. In SFT, we typically concatenate the question and the expected answer, providing both to the model as input-output pairs for supervised learning. In contrast, with GRPO, only the question is provided as input, and the model is tasked with generating both the reasoning and the final answer autonomously.\n",
        "\n",
        "This distinction is crucial because, in GRPO, the model's output‚Äîconsisting of the reasoning followed by the answer‚Äîis evaluated using reward functions. These rewards assess not only the correctness of the answer but also how well the model adheres to the specified output format and instructions, such as properly delineating reasoning and answers with predefined tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYpdURFAxKa2"
      },
      "source": [
        "# Preprocess Dataset\n",
        "Let's download openai/gsm8k dataset and convert each example into a dictionary {\"prompt\": ..., \"answer\": ...}  where\n",
        "- prompt field is a list of {\"role\": ..., \"content\": ...} where role is either \"system\" or \"user\" and content is the prompt.\n",
        "- answer field is the final numeric answer for the math problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVzK4bzSxKa2"
      },
      "source": [
        "Since one of the roles is system, it is a good time to finalize the system prompt that instructs the model how to generate its completions in a specific format. Specifically, we want the model to provide its reasoning enclosed within <thinking> and </thinking> tags, followed by the final answer enclosed within <answer> and </answer> tags. This formatting is critical, as the reward functions we design will rely on it to evaluate whether the model's completions adhere to our instructions. Of course, depending on the capabilities of the base model, it may not consistently follow the system prompt as intended. This is precisely where reinforcement learning (RL) comes in ‚Äî to reinforce and instill this structured format into the model's behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3As6kVwxKa2"
      },
      "outputs": [],
      "source": [
        "# Here is an example system prompt that we can use\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format and make sure the entire response is wrapped in <reasoning> and <answer> tags with no other text outside of these tags:\n",
        "<thinking>\n",
        "your reasoning goes here and do not use newlines so that the entire reasoning becomes a single paragraph\n",
        "</thinking>\n",
        "<answer>\n",
        "your answer goes here\n",
        "</answer>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOQaUNKxKa2"
      },
      "source": [
        "If you review the implementation of apply_chat_template in trl/data_utils.py, you'll notice that it expects both a \"prompt\" and a \"completion\" in the input. When provided, Hugging Face (HF) will automatically concatenate the prompt and completion to form the final input sequence for the model. However, this behavior is exactly what we want to avoid in our setup.\n",
        "\n",
        "Unlike Supervised Fine-Tuning (SFT), where exposing the answer in the completion is necessary for next-token prediction, our goal with reinforcement learning is fundamentally different. We do not want to show the answer to the model upfront. If we included the answer in the completion field, the model would simply learn to predict the next token based on the provided answer‚Äîdefeating the purpose of encouraging autonomous reasoning.\n",
        "\n",
        "What we actually want is for the model to generate both the reasoning and the final answer on its own, starting only from the question. To achieve this, we deliberately avoid using the \"completion\" field. Instead, we store the ground truth answer separately in an \"answer\" field, which is ignored by the trainer‚Äôs apply_chat_template function. This way, the model receives only the question as input, and its output can then be evaluated using reward functions that check both the quality of reasoning and the correctness of the answer‚Äîwithout ever having been exposed to the answer during generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "06cbF0W2xKa2"
      },
      "outputs": [],
      "source": [
        "#  Preprocess openai/gsm8k and apply chat template\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from trl import apply_chat_template                                   # thin wrapper around tokenizer.apply_chat_template\n",
        "import random, textwrap\n",
        "\n",
        "def gsm8k_to_chat(example):\n",
        "    \"\"\"\n",
        "    Convert a raw GSM8K row into:\n",
        "      {\n",
        "        \"prompt\": [ {\"role\":\"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\":\"user\",   \"content\": <question>} ],\n",
        "        \"answer\": <gold_integer_string>\n",
        "      }\n",
        "    The gold answer is **not** put in a 'completion' field, so the trainer will\n",
        "    pass only the 'prompt' list to apply_chat_template().\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": example[\"question\"].strip()}\n",
        "        ],\n",
        "        \"answer\": example[\"answer\"].strip()          # we keep it for rewards later\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
        "raw_test  = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "\n",
        "train_chat = raw_train.map(gsm8k_to_chat, remove_columns=raw_train.column_names)\n",
        "test_chat  = raw_test.map(gsm8k_to_chat,  remove_columns=raw_test.column_names)\n"
      ],
      "metadata": {
        "id": "b5yQBqNHa1KD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from statistics import mean\n",
        "\n",
        "def render_with_template(prompt_list, *, tokenize=False):\n",
        "    \"\"\"\n",
        "    Wrapper that handles both old and new tokenizer signatures.\n",
        "    If your tokenizer supports `add_generation_prompt`, we use it;\n",
        "    otherwise we call it without that kwarg.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return tokenizer.apply_chat_template(\n",
        "            prompt_list,\n",
        "            tokenize=tokenize,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except TypeError:\n",
        "        # older transformers version ‚Äì no add_generation_prompt kwarg\n",
        "        return tokenizer.apply_chat_template(\n",
        "            prompt_list,\n",
        "            tokenize=tokenize\n",
        "        )\n",
        "\n",
        "# --- show a single rendered example ---------------------------------------\n",
        "sample = train_chat[random.randint(0, len(train_chat) - 1)]\n",
        "rendered_text = render_with_template(sample[\"prompt\"], tokenize=False)\n",
        "print(\"---- rendered text sent to the model ----\")\n",
        "print(rendered_text[:800] + (\" ...\" if len(rendered_text) > 800 else \"\"))\n",
        "\n",
        "# --- compute token lengths for the whole training split --------------------\n",
        "def prompt_length(example):\n",
        "    ids = render_with_template(example[\"prompt\"], tokenize=True)\n",
        "    return len(ids)\n",
        "\n",
        "lengths = [prompt_length(ex) for ex in train_chat]\n",
        "\n",
        "print(f\"Average prompt length: {mean(lengths):.1f}\")\n",
        "print(f\"Max prompt length: {max(lengths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBWLjGrxcpeV",
        "outputId": "f443eb12-91fe-4f0c-9438-297e45296974"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- rendered text sent to the model ----\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "Respond in the following format and make sure the entire response is wrapped in <reasoning> and <answer> tags with no other text outside of these tags:\n",
            "<thinking>\n",
            "your reasoning goes here and do not use newlines so that the entire reasoning becomes a single paragraph\n",
            "</thinking>\n",
            "<answer>\n",
            "your answer goes here\n",
            "</answer><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Mark is looking to buy a total of 12 pieces of fruit at the store. He has already chosen 3 apples. He has also selected a bunch of bananas containing 4 bananas. How many oranges does he need to pick out to have 12 total pieces of fruit?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "Average prompt length: 161.3\n",
            "Max prompt length: 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VBXcYoTxKa2"
      },
      "source": [
        "# Reward Functions\n",
        "GRPO needs a set of reward functions to determine how well model's completions adhere to our expectations. Let's add some reward functions. Note that the GRPO trainer sends the following input arguments, among others, to each reward function:\n",
        "- prompts,\n",
        "- completions,\n",
        "- answer\n",
        "\n",
        "You, as the designer, can choose to use them all or use whichever you need and pass the rest to **kwargs.\n",
        "\n",
        "- Example: def reward_func(prompts, completions, answer, **kwargs) ---> uses all 3\n",
        "- Example: def reward_func(completions, answer, **kwargs) ---> uses only completion\n",
        "\n",
        "Also, you should make sure  there is a 1-to-1 correspondence between completions and returned rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xg6p0zESxKa3"
      },
      "outputs": [],
      "source": [
        "# Example: a reward function that penalizes the completion if the completion length is longer than 100 tokens\n",
        "def length_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [0.0 if len(tokenizer(contents[i])) < 100 else -0.5 for i in range(len(completions))]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reward function to extract the final answer from model completions and check if it is an integer."
      ],
      "metadata": {
        "id": "g-GETacyl9pS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o1-amxQsxKa3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "_ANS_TAG_RE = re.compile(r\"<answer>\\s*(.*?)\\s*</answer>\", re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "def int_reward_func(completions: List[list], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    Reward = +1.0  if the model's <answer>‚Ä¶</answer> tag contains\n",
        "                     a legal integer (optionally signed).\n",
        "             -1.0  otherwise.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    ‚Ä¢ `completions` is a list of *chat turns*; each element is itself a\n",
        "      list of messages.  We follow the same convention used in the earlier\n",
        "      `length_reward_func`, extracting the first (and usually only) message\n",
        "      via completions[i][0][\"content\"].\n",
        "\n",
        "    ‚Ä¢ This function deliberately ignores the ground‚Äëtruth `answer`;\n",
        "      it checks only *format* + *integer‚Äëparsability* and leaves correctness\n",
        "      to a separate reward.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        text = comp[0][\"content\"] if comp and isinstance(comp[0], dict) else \"\"\n",
        "        m = _ANS_TAG_RE.search(text)\n",
        "        if m:\n",
        "            candidate = m.group(1).strip()\n",
        "            # integer test: optional sign followed by digits\n",
        "            rewards.append( 1.0 if re.fullmatch(r\"[+-]?\\d+\", candidate) else -1.0 )\n",
        "        else:\n",
        "            # no <answer> tag at all = strong penalty\n",
        "            rewards.append(-1.0)\n",
        "    return rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reward function to ensure the completion follows this pattern: `^<thinking>\\n.*?\\n</thinking>\\n<answer>\\n.*?\\n</answer>$`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FVoE4vnlpSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KYYYUuF7xKa3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "# Strict pattern: exactly two blocks in order, each wrapped in its XML tag,\n",
        "# with *single* newline after each tag open/close (per your example pattern).\n",
        "_STRICT_RE = re.compile(\n",
        "    r\"^<thinking>\\n.*?\\n</thinking>\\n<answer>\\n.*?\\n</answer>$\",\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "def strict_format_reward_func(completions: List[list], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    +1.0  if the completion matches the exact multi‚Äëline template\n",
        "          <thinking>\\n‚Ä¶\\n</thinking>\\n<answer>\\n‚Ä¶\\n</answer>\n",
        "    -1.0  otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for c in completions:\n",
        "        txt = c[0][\"content\"] if c and isinstance(c[0], dict) else \"\"\n",
        "        rewards.append(1.0 if _STRICT_RE.fullmatch(txt) else -1.0)\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reward function that penalizes the model if the completion has any tokens after the answer tag."
      ],
      "metadata": {
        "id": "QpqA1cdtlhLc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ObboG69zxKa3"
      },
      "outputs": [],
      "source": [
        "_END_TAG_RE = re.compile(r\"</answer>\\s*(.*)$\", re.DOTALL)\n",
        "\n",
        "def xmlcount_reward_func(completions: List[list], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    0.0   if *no* text remains after the closing </answer> tag.\n",
        "    -0.5  if any non‚Äëwhitespace characters appear afterwards.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for c in completions:\n",
        "        txt = c[0][\"content\"] if c and isinstance(c[0], dict) else \"\"\n",
        "        match = _END_TAG_RE.search(txt)\n",
        "        if not match:                      # no </answer> tag at all ‚Üí penalise\n",
        "            rewards.append(-0.5)\n",
        "        else:\n",
        "            tail = match.group(1)\n",
        "            rewards.append(0.0 if tail.strip() == \"\" else -0.5)\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reward function to extract the final answer from model completions and check if it is correct."
      ],
      "metadata": {
        "id": "6mn51NidmBF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sE-V0R-CFEdS"
      },
      "outputs": [],
      "source": [
        "_ANS_RE   = re.compile(r\"<answer>\\s*([+-]?\\d+)\\s*</answer>\", re.I | re.S)\n",
        "FINAL_RE  = re.compile(r\"####\\s*([+-]?\\d+)\", re.S)\n",
        "\n",
        "def extract_gold(ans_str):\n",
        "    m = FINAL_RE.search(ans_str)\n",
        "    return m.group(1).lstrip(\"+\").lstrip(\"0\") if m else None\n",
        "\n",
        "def answer_correctness_func(\n",
        "    prompts: List[list],\n",
        "    completions: List[list],\n",
        "    answer: List[str],\n",
        "    **kwargs\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    +1.0 if the integer inside <answer>...</answer> matches the integer\n",
        "         after '####' in the GSM8K gold string.\n",
        "     0.0 otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp, gold_str in zip(completions, answer):\n",
        "        # model prediction\n",
        "        txt       = comp[0][\"content\"] if comp and isinstance(comp[0], dict) else \"\"\n",
        "        pred_match = _ANS_RE.search(txt)\n",
        "        pred_num   = pred_match.group(1).lstrip(\"+\").lstrip(\"0\") if pred_match else None\n",
        "\n",
        "        # ground truth\n",
        "        gold_num = extract_gold(gold_str)\n",
        "\n",
        "        rewards.append(1.0 if pred_num is not None and pred_num == gold_num else 0.0)\n",
        "\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "### GRPO config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig\n",
        "\n",
        "grpo_cfg = GRPOConfig(\n",
        "    # ---------- folders / logging ----------\n",
        "    output_dir                  = \"ckpts/gsm8k_grpo_lora\",\n",
        "    logging_steps               = 10,\n",
        "    evaluation_strategy         = \"steps\",\n",
        "    eval_steps                  = 250,\n",
        "    save_strategy               = \"epoch\",\n",
        "    bf16                        = True,          # if we are using A100 / H100\n",
        "\n",
        "    # ---------- optimiser ----------\n",
        "    learning_rate               = 2e-4,\n",
        "    weight_decay                = 0.01,\n",
        "    lr_scheduler_type           = \"cosine\",\n",
        "    warmup_steps                = 500,\n",
        "\n",
        "    # ---------- batching ----------\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 8,              # ‚áí effective 32\n",
        "\n",
        "    # ---------- GRPO specifics ----------\n",
        "    num_generations             = 4,              # group size‚ÄØG\n",
        "    beta                        = 0.02,           # KL coef\n",
        "    temperature                 = 0.7,\n",
        "    top_p                       = 0.9,\n",
        "\n",
        "    # ---------- NEW length knobs ----------\n",
        "    max_prompt_length           = 512,            # fits every GSM8K question + system msg\n",
        "    max_completion_length       = 256,            # ample for chain‚Äëof‚Äëthought + answer\n",
        "    label_names=[],          # suppress Trainer‚Äôs guess‚Äëand‚Äëwarn\n",
        "\n",
        ")\n",
        "print(\"GRPOConfig built OK ‚úÖ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlcrCWFSzjiH",
        "outputId": "a51762c4-da0d-480b-d180-c11b5c1b1d8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRPOConfig built OK ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSs_-O9KxKa3"
      },
      "source": [
        "# üß† Understanding GRPO Trainer Workflow\n",
        "\n",
        "While setting up a GRPO config is straightforward, understanding *what each hyperparameter means* and *how the underlying training logic works* is essential. This guide walks through the GRPO training loop, highlighting the key code paths in `trl/trainer/grpo_trainer.py`.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ GRPO Trainer in Context\n",
        "\n",
        "GRPO is built on top of the TRL `Trainer` class, but it customizes key parts of the training pipeline by overriding several critical methods:\n",
        "\n",
        "- `compute_loss`\n",
        "- `prepare_input`\n",
        "\n",
        "> These functions are the heart of GRPO's training logic. Let‚Äôs look at what happens in each of them.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ GRPO Workflow in `train_step`\n",
        "\n",
        "In each `train_step`:\n",
        "\n",
        "1. `Trainer.train_step()` in the parent class first calls `prepare_input()` in the GRPO trainer.\n",
        "2. Then it calls `compute_loss()` with the output of `prepare_input()`.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ `prepare_input()` Logic\n",
        "\n",
        "This function handles **prompt generation**, **inference**, and **buffering** of completions. Here's a breakdown:\n",
        "\n",
        "### ‚úÖ High-Level Behavior:\n",
        "\n",
        "- Called **once per `self.num_iterations`** to generate fresh completions.\n",
        "- **Buffering** is used to reuse completions across multiple gradient updates.\n",
        "\n",
        "> For example:\n",
        "> - `num_iterations = 4`: generate completions every 4 global steps. Each global step can consists of k grad accumulation steps.\n",
        "> - `gradient_accumulation_steps = 5`: buffer stores 5 sets of completions.\n",
        "\n",
        "If `num_iterations = 1`, completions are not reused, and buffering may be unnecessary.\n",
        "\n",
        "### üß¨ Core Logic (Simplified):\n",
        "\n",
        "```python\n",
        "@profiling_decorator\n",
        "def _prepare_inputs(self, inputs: dict) -> dict:\n",
        "    mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
        "    if mode == \"train\":\n",
        "        if self.state.global_step % self.num_iterations == 0:\n",
        "            inputs = self._generate_and_score_completions(inputs)\n",
        "            self._buffered_inputs[self._step % self.args.gradient_accumulation_steps] = inputs\n",
        "        else:\n",
        "            inputs = self._buffered_inputs[self._step % self.args.gradient_accumulation_steps]\n",
        "        self._step += 1\n",
        "    else:\n",
        "        inputs = self._generate_and_score_completions(inputs)\n",
        "    return inputs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Prompt Construction\n",
        "\n",
        "- Conversations are stored in the `\"prompt\"` field.\n",
        "- The `apply_chat_template()` function is used to convert this into a generation-ready string.\n",
        "  \n",
        "```python\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    example[\"prompt\"], tools=tools, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Inference (Completions Generation)\n",
        "You can check the code for vllm path which is more efficient. I didn't get a chance to look into it.\n",
        "\n",
        "for non-vllm path:\n",
        "\n",
        "- each rank tokenizes its prompts and truncates them by max_prompt_length\n",
        "    \n",
        "- each rank will send its text prompts to the on-device llm\n",
        "\n",
        "    ```python\n",
        "    prompt_completion_ids[:,-5:] => the last 5 tokens for 6 generations made on GPU0. With batched inference, we append EOS (128009) for early completed answers.\n",
        "        tensor([[128009, 128009, 128009, 128009, 128009],\n",
        "                [128009, 128009, 128009, 128009, 128009],\n",
        "                [128009, 128009, 128009, 128009, 128009],\n",
        "                [   279,   1176,    220,    605,  14741],\n",
        "                [   220,    430,    568,  11021,    220],\n",
        "                [128009, 128009, 128009, 128009, 128009]], device='cuda:0')\n",
        "    ```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Completion Masking\n",
        "\n",
        "Completion masks identify **valid** (non-EOS) completions.\n",
        "\n",
        "Each rank compute completion masks based on the first occurrence of EOS. The following is the mask for completions above.\n",
        "\n",
        "```python\n",
        "completion_mask[:,-5:]\n",
        "    tensor([[0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0],\n",
        "            [0, 0, 0, 0, 0],\n",
        "            [1, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 1],\n",
        "            [0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Reward Computation and Normalization\n",
        "\n",
        "Each rank:\n",
        "- Decodes completions\n",
        "- Computes reward for (prompt, completion)\n",
        "- Gathers rewards from other ranks, because it's possible for a given prompt to have its replica across GPUs. See the sampler.\n",
        "- Normalizes rewards by mean/std => This gives us advantages $A(s, a)$\n",
        "- Discards completions for prompts it doesn‚Äôt own (called *alien prompts*)\n",
        "\n",
        "Let‚Äôs walk through a concrete example to understand prompt replication and reward normalization across GPUs.\n",
        "\n",
        "üßÆ Setup:\n",
        "\n",
        "- 3 GPUs (Ranks)\n",
        "- Batch size per GPU: 8\n",
        "- Number of generations per prompt: 6\n",
        "\n",
        "üì¶ Total Prompts Processed per Iteration:\n",
        "- Total batch size = 3 GPUs √ó 8 prompts = 24 prompts\n",
        "- Since each prompt needs 6 completions, we can only afford to have:\n",
        " - 24 / 6 = 4 unique prompts\n",
        "\n",
        "Each prompt is replicated 6 times across all ranks.\n",
        "\n",
        "üîÅ Prompt Distribution Example:\n",
        "On GPU 0 (Rank 0):\n",
        "- 8 total prompts:\n",
        " - 6 replicas of Prompt #1\n",
        " - 2 replicas of Prompt #2\n",
        "\n",
        "> Note: Other ranks may hold the remaining replicas of Prompt #2 and additional prompts.\n",
        "\n",
        "üéØ Reward Normalization:\n",
        "- For Prompt #1:\n",
        "    - All 6 completions are on GPU 0.\n",
        "    - ‚úÖ Rank 0 can compute mean and std of rewards locally.\n",
        "\n",
        "\n",
        "- For Prompt #2:\n",
        "    - Rank 0 has only 2 out of 6 completions.\n",
        "    - ‚ùå It cannot compute accurate reward statistics alone.\n",
        "    - ‚úÖ Needs to gather the remaining 4 rewards from other ranks.\n",
        "\n",
        "\n",
        "> That's why all gather is needed so each rank has access to the required replicas\n",
        "---\n",
        "\n",
        "## üßæ Logprob Computation\n",
        "\n",
        "\n",
        "- We generate rollouts (prompts + completions) are stored per grad accumulation step (step % G) during this first iteration.\n",
        "\n",
        "- These buffered inputs are reused for the remaining iterations within that num_iterations window.\n",
        "    - Buffering occurs only during the first num_iterations step (i.e., at global_step % num_iterations == 0).\n",
        "\n",
        "- Logprobs for old policy and ref policy are computed during the first iteration only (i.e., when completions are freshly generated).\n",
        "\n",
        "- Logprobs for current policy are computed in every gradient accumulation step, including when using buffered inputs.\n",
        "\n",
        "\n",
        "> Note: old and current policy logprobs will be identical when `num_iterations = 1`.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ `compute_loss()` Breakdown\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{GRPO}} = - \\mathbb{E}_{(s, a)} \\left[ \\frac{\\pi(a|s)}{\\pi_{\\text{old}}(a|s)} A(s, a) \\right] + \\beta \\cdot \\text{KL}[\\pi(\\cdot|s) \\| \\pi_{\\text{ref}}(\\cdot|s)]\n",
        "$$\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Concatenate** `prompt_ids + completion_ids`.\n",
        "2. Run a **forward pass** through the old policy to compute $\\pi_{old}(a|s)$\n",
        "    - This actually happens only once at the first iteration when we create the rollout.\n",
        "3. Run a **forward pass** through the ref policy to compute  $\\pi_{ref}(a|s)$\n",
        "    - This actually happens only once at the first iteration when we create the rollout.\n",
        "    - ref model is the original model without LoRA adapters\n",
        "4. Run a **forward pass** through the current policy to compute $\\pi(a|s)$\n",
        "    - Needed only if number_iterations > 1; otherwise the same as old policy\n",
        "5. Compute **KL loss**: between $\\pi(a|s)$ and $\\pi_{ref}(a|s)$.\n",
        "\n",
        "6. Compute **advantage-weighted logprobs**: $\\frac{\\pi(a|s) }{ \\pi_{old}(a|s)} * A(s, a)$\n",
        "\n",
        "---\n",
        "\n",
        "## üìã GRPO Trainer Workflow Summary\n",
        "\n",
        "| Component             | What It Does                                                                 | Why It Matters                                                                 |\n",
        "|----------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
        "| GRPO Trainer Class   | Extends the TRL Trainer, overrides `prepare_input` and `compute_loss`       | Customizes the loss and input preparation for rollout reuse and reward learning |\n",
        "| `train_step`         | Calls `prepare_input` then `compute_loss`                                    | Controls how and when rollouts and gradients are processed                     |\n",
        "| `prepare_input()`    | Generates and buffers rollouts once every `num_iterations` steps             | Allows reuse of expensive rollouts across multiple updates                     |\n",
        "| Prompt Construction  | Uses `apply_chat_template` to create generation-ready input                  | Ensures the model sees correctly formatted conversational prompts             |\n",
        "| Inference            | Uses on-device or vLLM backend to generate completions                       | Provides actions for which logprobs and rewards are computed                   |\n",
        "| Completion Masking   | Identifies valid (non-EOS) completions                                       | Ensures reward/logprob computation only applies to meaningful tokens          |\n",
        "| Reward Normalization | Aggregates and normalizes rewards across GPUs for each prompt                | Yields correct advantage estimates across distributed setup                   |\n",
        "| Logprob Computation  | Computes logprobs of old, ref, and current policy                            | Needed for KL and advantage-weighted loss; reused if `num_iterations > 1`     |\n",
        "| `compute_loss()`     | Combines KL divergence and advantage-weighted logprob ratio                 | Drives the optimization update direction for the policy                       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title An easier setting on faster training\n",
        "grpo_cfg.num_generations          = 2\n",
        "grpo_cfg.max_completion_length    = 128\n",
        "grpo_cfg.num_iterations           = 4\n",
        "grpo_cfg.per_device_train_batch_size = 8\n",
        "grpo_cfg.gradient_accumulation_steps = 4   # keeps effective batch = 32\n"
      ],
      "metadata": {
        "id": "sSq10i8wKR5N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_cfg.deepspeed = \"ds_zero2.json\"     # or zero3\n"
      ],
      "metadata": {
        "id": "J0vi_6tRQ8yR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_chat), len(test_chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyrKYykMSdzC",
        "outputId": "e1d33ded-6d82-4bff-d608-2c67bacba64a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7473, 1319)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Eary experimentation setting\n",
        "small_train = train_chat.shuffle(seed=42).select(range(3000))  # 2k examples\n",
        "grpo_cfg.num_train_epochs = 1\n"
      ],
      "metadata": {
        "id": "SsNpPnnOQznj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOTrainer\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model           = model,             # your LoRA‚Äëpatched Llama‚Äë3‚Äë8B\n",
        "    args            = grpo_cfg,          # <-- positional name is 'args'\n",
        "    processing_class= tokenizer,         # tokenizer goes here (alias kept for BW compat) :contentReference[oaicite:6]{index=6}\n",
        "    reward_funcs    = [\n",
        "        strict_format_reward_func,\n",
        "        xmlcount_reward_func,\n",
        "        int_reward_func,\n",
        "        answer_correctness_func\n",
        "    ],\n",
        "    # train_dataset   = small_train,     # uncomment if only want to do a quick experiment\n",
        "    train_dataset   = train_chat,\n",
        "\n",
        "    eval_dataset    = test_chat,\n",
        "    peft_config     = lora_config,       # only if you re‚Äëinstantiate; else drop\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVsVFdO_qxV2",
        "outputId": "bf106db1-0285-4526-eea5-e031ae79e1d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'meta-llama/Meta-Llama-3.1-8B-Instruct' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"llama3_8b_gsm8k_grpo_lora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "OBlS61F3C3Xd",
        "outputId": "e144e822-20ef-45c3-a1df-09794f94842c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjhuang-daniel\u001b[0m (\u001b[33mjhuang-daniel-siclarity\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_070356-05vwnr2l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jhuang-daniel-siclarity/huggingface/runs/05vwnr2l' target=\"_blank\">ckpts/gsm8k_grpo_lora</a></strong> to <a href='https://wandb.ai/jhuang-daniel-siclarity/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jhuang-daniel-siclarity/huggingface' target=\"_blank\">https://wandb.ai/jhuang-daniel-siclarity/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jhuang-daniel-siclarity/huggingface/runs/05vwnr2l' target=\"_blank\">https://wandb.ai/jhuang-daniel-siclarity/huggingface/runs/05vwnr2l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='751' max='5604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 751/5604 5:25:23 < 35:08:15, 0.04 it/s, Epoch 0.40/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.022100</td>\n",
              "      <td>0.046335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.020300</td>\n",
              "      <td>0.031951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/330 1:11:39 < 35:59, 0.05 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1001' max='5604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1001/5604 7:50:04 < 36:05:55, 0.04 it/s, Epoch 0.54/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.022100</td>\n",
              "      <td>0.046335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.020300</td>\n",
              "      <td>0.031951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.039535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/330 34:48 < 1:12:13, 0.05 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download the trained model to local machine\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Save the model checkpoint locally\n",
        "!zip -r /content/llama3_8b_gsm8k_grpo_lora.zip /content/llama3_8b_gsm8k_grpo_lora\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/llama3_8b_gsm8k_grpo_lora.zip')\n"
      ],
      "metadata": {
        "id": "UdKTHDgfkUmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: clear memory\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "# del trainer"
      ],
      "metadata": {
        "id": "KIyNYNexJmEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Evaluate the model\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "JOfYkMqhQdZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2de9666-858c-4f38-fbe9-9e7428277b40"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "The token `reasoning-llama3` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `reasoning-llama3`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6sLQj__xKa4"
      },
      "outputs": [],
      "source": [
        "#  load a LoRA checkpoint\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id   = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "lora_ckpt_path  = \"llama3_8b_gsm8k_grpo_lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "\n",
        "# IMPORTANT: load the *full‚Äëprecision* base so merge math is exact\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=\"auto\",       # fp16/bf16 on GPU ‚Üí fine for merge\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, lora_ckpt_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload()       # returns a plain `AutoModel`\n"
      ],
      "metadata": {
        "id": "rUL9_I_RT7Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this right after we reload the tokenizer / model to make Llama3 happy\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    # If the model and tokenizer sizes now differ, resize embeddings:\n",
        "    merged_model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "lSDDNIuNhrC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "#  load test dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# If you already mapped the test split earlier, you can skip the next two lines\n",
        "raw_test  = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")   # 1‚ÄØ319 rows\n",
        "test_chat = raw_test.map(gsm8k_to_chat, remove_columns=raw_test.column_names)\n",
        "\n",
        "print(\"Loaded\", len(test_chat), \"examples for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMYLvKvWxKa4"
      },
      "outputs": [],
      "source": [
        "import re, torch, textwrap, random\n",
        "from tqdm import tqdm\n",
        "\n",
        "answer_re = re.compile(r\"<answer>\\s*([+-]?\\d+)\\s*</answer>\", re.I | re.S)\n",
        "\n",
        "def extract_num(text):\n",
        "    m = answer_re.search(text)\n",
        "    return m.group(1).lstrip(\"+\").lstrip(\"0\") if m else None\n",
        "\n",
        "final_re = re.compile(r\"####\\s*([+-]?\\d+)\", re.S)\n",
        "\n",
        "def extract_gold(ans_str):\n",
        "    m = final_re.search(ans_str)\n",
        "    return m.group(1).lstrip(\"+\").lstrip(\"0\") if m else None\n",
        "\n",
        "def evaluate(test_ds, model, tokenizer, batch_size=8):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total   = 0\n",
        "    samples = []\n",
        "\n",
        "    for start in tqdm(range(0, len(test_ds), batch_size)):\n",
        "        # .select() returns a Dataset;  .to_list() turns it into list‚Äëof‚Äëdicts\n",
        "        batch_rows = test_ds.select(range(start, min(start+batch_size, len(test_ds)))).to_list()\n",
        "\n",
        "        prompt_texts = [\n",
        "            tokenizer.apply_chat_template(ex[\"prompt\"], add_generation_prompt=True, tokenize=False)\n",
        "            for ex in batch_rows\n",
        "        ]\n",
        "        inputs = tokenizer(prompt_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.0,     # deterministic\n",
        "                top_p=1.0,\n",
        "                do_sample=False,\n",
        "            )\n",
        "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        for ex, tex in zip(batch_rows, decoded):\n",
        "            pred = extract_num(tex)\n",
        "            gold = extract_gold(ex[\"answer\"])\n",
        "            total += 1\n",
        "            if pred == gold:\n",
        "                correct += 1\n",
        "            if len(samples) < 10:\n",
        "                samples.append((ex[\"prompt\"][1][\"content\"], pred, gold, tex[:160]))\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, samples\n",
        "\n",
        "acc, samples = evaluate(test_chat, merged_model, tokenizer)\n",
        "print(f\"\\nAccuracy on GSM8K test split: {acc:.2%}  ({int(acc*len(test_chat))}/{len(test_chat)})\")\n",
        "\n",
        "print(\"\\n--- sample completions ---\")\n",
        "for q,p,g,snip in samples:\n",
        "    print(textwrap.shorten(q, 70), \"| pred:\", p, \"| gold:\", g)\n",
        "    print(\"  ‚Ü≥\", textwrap.shorten(snip, 120), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"merged_llama3_gsm8k\"\n",
        "merged_model.save_pretrained(output_dir, safe_serialization=True)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "aawgLFgpT8Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bitsandbytes import Int8Params\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
        "quant_model = AutoModelForCausalLM.from_pretrained(output_dir, quantization_config=bnb_cfg)\n",
        "quant_model.save_pretrained(output_dir + \"_4bit\")\n"
      ],
      "metadata": {
        "id": "1xJhPfB_T-yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"lordChipotle/Llama3GRPOReasoning\"\n",
        "merged_model.push_to_hub(repo_name, private=False)    # or quant_model.push_to_hub(...)\n",
        "tokenizer.push_to_hub(repo_name)\n"
      ],
      "metadata": {
        "id": "lE8gM7u3UAvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGkQoBhlxKa4"
      },
      "source": [
        "# Next?\n",
        "Perhaps extending this notebook for something similar to [medical reasoning](https://github.com/matthewchung74/qwen_2_5_3B_GRPO_medical_thinking/blob/main/Qwen2_5_(3B)_GRPO.ipynb)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iu3DeosOzPmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83a67a8aae4a42b1815663f1ded9e027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0149d407f8ee40bb8f3caf0ad88d37b7",
              "IPY_MODEL_ed3c465c550c44a1ab0e8015efdc40fc",
              "IPY_MODEL_4e4f3ef2b3224c85afd8f4e49d8bdb80"
            ],
            "layout": "IPY_MODEL_0b2929ca228442bdb7eb1f2a9985aa2e"
          }
        },
        "0149d407f8ee40bb8f3caf0ad88d37b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_539b50bc7f53458eafd0a760e57fb986",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9fd4eb548d994dc1935edc1647c77d4b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ed3c465c550c44a1ab0e8015efdc40fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7089bab69e4166bed11ce5bf4c362d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_711e44021c684adcbcde3fa287951cf2",
            "value": 4
          }
        },
        "4e4f3ef2b3224c85afd8f4e49d8bdb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a880e478e0d439c8e73afb9ee90868a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e72b75bcd46649d8a35c236a3bedf0c4",
            "value": "‚Äá4/4‚Äá[00:04&lt;00:00,‚Äá‚Äá1.06s/it]"
          }
        },
        "0b2929ca228442bdb7eb1f2a9985aa2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539b50bc7f53458eafd0a760e57fb986": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd4eb548d994dc1935edc1647c77d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7089bab69e4166bed11ce5bf4c362d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711e44021c684adcbcde3fa287951cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a880e478e0d439c8e73afb9ee90868a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e72b75bcd46649d8a35c236a3bedf0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}